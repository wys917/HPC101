# Qwen3-8B å‚æ•°é‡å’Œæ˜¾å­˜å ç”¨ç†è®ºåˆ†æ

## ğŸ“Š ç†è®ºå‚æ•°é‡è®¡ç®—

### æ¨¡å‹é…ç½®å‚æ•°
```python
vocab_size = 152,064
hidden_size = 4,096  
intermediate_size = 11,008
num_hidden_layers = 32
num_attention_heads = 32
num_key_value_heads = 8  # GQAé…ç½®
```

### 1. Word Embeddingå±‚
```
vocab_size Ã— hidden_size = 152,064 Ã— 4,096 = 622,854,144
çº¦ 623M å‚æ•°
```

### 2. å•ä¸ªDecoder Layerå‚æ•°

#### 2.1 Self-Attentionéƒ¨åˆ†
```python
# QKVæŠ•å½±å‚æ•°
q_dim = 32 Ã— (4096/32) = 32 Ã— 128 = 4,096
kv_dim = 8 Ã— (4096/32) = 8 Ã— 128 = 1,024  
qkv_params = 4,096 Ã— (4,096 + 1,024 + 1,024) = 4,096 Ã— 6,144 = 25,165,824

# è¾“å‡ºæŠ•å½±å‚æ•°
o_proj_params = 4,096 Ã— 4,096 = 16,777,216

# Q/Kå½’ä¸€åŒ–å‚æ•°
qk_norm_params = 2 Ã— 4,096 = 8,192

# Self-Attentionæ€»è®¡
attention_params = 25,165,824 + 16,777,216 + 8,192 = 41,951,232
çº¦ 42M å‚æ•°
```

#### 2.2 MLP (SwiGLU) éƒ¨åˆ†
```python
# GateæŠ•å½±
gate_params = 4,096 Ã— 11,008 = 45,088,768

# UpæŠ•å½±  
up_params = 4,096 Ã— 11,008 = 45,088,768

# DownæŠ•å½±
down_params = 11,008 Ã— 4,096 = 45,088,768

# MLPæ€»è®¡
mlp_params = 45,088,768 Ã— 3 = 135,266,304
çº¦ 135M å‚æ•°
```

#### 2.3 LayerNormå‚æ•°
```python
# æ¯å±‚æœ‰2ä¸ªLayerNorm
layernorm_params = 2 Ã— 4,096 = 8,192
```

#### 2.4 å•å±‚æ€»è®¡
```python
single_layer = 41,951,232 + 135,266,304 + 8,192 = 177,225,728
çº¦ 177M å‚æ•°
```

### 3. æ‰€æœ‰Decoder Layers
```python
all_layers = 177,225,728 Ã— 32 = 5,671,223,296
çº¦ 5.67B å‚æ•°
```

### 4. Language Model Head
```python
lm_head = 4,096 Ã— 152,064 = 622,854,144
çº¦ 623M å‚æ•°
```

### 5. Final LayerNorm
```python
final_norm = 4,096 å‚æ•°
```

### 6. ç†è®ºæ€»å‚æ•°é‡
```python
total = 622,854,144 + 5,671,223,296 + 622,854,144 + 4,096
      = 6,916,935,680
      â‰ˆ 6.92B å‚æ•°
```

## ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨è®¡ç®—

### æ¨¡å‹å‚æ•°æ˜¾å­˜ (bfloat16)
```python
å‚æ•°æ•°é‡: 6,916,935,680
æ¯å‚æ•°å­—èŠ‚: 2 (bfloat16)
æ¨¡å‹æ˜¾å­˜ = 6,916,935,680 Ã— 2 = 13,833,871,360 bytes
       = 13.83 GB
```

### æ¨ç†æ—¶é¢å¤–æ˜¾å­˜å¼€é”€
```python
# å‡è®¾: batch_size=1, seq_len=100

# æ¿€æ´»å€¼æ˜¾å­˜ (æ¯å±‚)
activation_per_layer = 1 Ã— 100 Ã— 4,096 Ã— 2 = 819,200 bytes
total_activation = 819,200 Ã— 32 = 26,214,400 bytes â‰ˆ 0.026 GB

# æ³¨æ„åŠ›çŸ©é˜µæ˜¾å­˜
attention_matrix = 1 Ã— 32 Ã— 100 Ã— 100 Ã— 2 = 640,000 bytes â‰ˆ 0.0006 GB

# KV Cacheæ˜¾å­˜ (è‡ªå›å½’ç”Ÿæˆæ—¶)
kv_cache = 2 Ã— 32 Ã— 8 Ã— 100 Ã— 128 Ã— 2 â‰ˆ 13.1 MB â‰ˆ 0.013 GB

# æ€»æ¨ç†æ˜¾å­˜ä¼°ç®—
inference_total â‰ˆ 13.83 + 0.026 + 0.013 + 1.0(buffer) â‰ˆ 14.87 GB
```

## ğŸ” é¢„æœŸå·®å¼‚åˆ†æ

### ç†è®ºvså®é™…å¯èƒ½å­˜åœ¨çš„å·®å¼‚

1. **å†…å­˜å¯¹é½å¼€é”€**
   - GPUè¦æ±‚å†…å­˜æŒ‰256å­—èŠ‚æˆ–æ›´å¤§è¾¹ç•Œå¯¹é½
   - å¯èƒ½å¢åŠ  2-5% çš„é¢å¤–å¼€é”€

2. **PyTorchæ¡†æ¶å¼€é”€**
   - å¼ é‡å…ƒæ•°æ® (shape, stride, deviceä¿¡æ¯)
   - è®¡ç®—å›¾ç›¸å…³ä¿¡æ¯
   - çº¦ 100-500 MB çš„åŸºç¡€å¼€é”€

3. **CUDAä¸Šä¸‹æ–‡å¼€é”€**
   - GPUé©±åŠ¨åˆå§‹åŒ–
   - CUDAè¿è¡Œæ—¶åº“
   - çº¦ 200-800 MB

4. **åŠ¨æ€å†…å­˜åˆ†é…**
   - PyTorchå†…å­˜æ± ç®¡ç†
   - ç¢ç‰‡åŒ–å¯¼è‡´çš„æµªè´¹
   - å¯èƒ½å¢åŠ  10-20% çš„å¼€é”€

5. **ç²¾åº¦è€ƒè™‘**
   - æŸäº›ä¸­é—´è®¡ç®—å¯èƒ½ä½¿ç”¨float32
   - æ¢¯åº¦å’Œä¼˜åŒ–å™¨çŠ¶æ€(å¦‚æœå­˜åœ¨)

## ğŸ“ˆ é¢„æœŸç»“æœ

### å‚æ•°é‡å¯¹æ¯”
- **ç†è®ºå€¼**: çº¦ 6.92B å‚æ•°
- **å®é™…å€¼**: åº”è¯¥éå¸¸æ¥è¿‘ï¼Œå·®å¼‚ < 1%

### æ˜¾å­˜å ç”¨å¯¹æ¯”  
- **ç†è®ºæ¨¡å‹æ˜¾å­˜**: çº¦ 13.83 GB
- **å®é™…æ¨¡å‹æ˜¾å­˜**: çº¦ 14.5-15.5 GB (è€ƒè™‘å¼€é”€)
- **æ¨ç†æ€»æ˜¾å­˜**: çº¦ 15-17 GB (åŒ…å«æ¿€æ´»å€¼ç­‰)

### GPUæ˜¾å­˜è¦æ±‚
- **æœ€å°è¦æ±‚**: 16 GB (å¦‚ V100)
- **æ¨èé…ç½®**: 24 GB æˆ–æ›´å¤§
- **å®é™…ä½¿ç”¨**: é¢„è®¡ 15-17 GB

è¿™ä¸ªåˆ†æä¸ºæˆ‘ä»¬æä¾›äº†åŸºå‡†ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹å®é™…æµ‹è¯•çš„ç»“æœå¦‚ä½•ï¼
