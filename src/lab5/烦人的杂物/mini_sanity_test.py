"""
ç®€å•çš„å®ç°æµ‹è¯•ï¼Œä¸ä¾èµ–é¢„è®­ç»ƒæ¨¡å‹
"""
import torch
import torch.nn as nn
from modules.attention import Qwen3Attention
from modules.mlp import Qwen3MLP
from modules.config import Qwen3Config


def test_attention():
    """æµ‹è¯•æ³¨æ„åŠ›æ¨¡å—"""
    print("æµ‹è¯• Qwen3Attention æ¨¡å—...")
    
    # åˆ›å»ºé…ç½®
    config = Qwen3Config(
        hidden_size=512,
        num_attention_heads=8,
        num_key_value_heads=8,
        max_position_embeddings=2048,
        rms_norm_eps=1e-6,
        rope_theta=10000.0
    )
    
    # åˆ›å»ºæ¨¡å—
    attention = Qwen3Attention(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 10
    hidden_size = config.hidden_size
    
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    try:
        output = attention(hidden_states)
        print(f"âœ… æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•é€šè¿‡")
        print(f"   è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        expected_shape = (batch_size, seq_len, hidden_size)
        if output.shape == expected_shape:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âœ… é€šè¿‡")
        else:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âŒ å¤±è´¥ï¼ŒæœŸæœ› {expected_shape}ï¼Œå¾—åˆ° {output.shape}")
            
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–Inf
        if torch.isnan(output).any():
            print(f"   æ•°å€¼æ£€æŸ¥: âŒ è¾“å‡ºåŒ…å«NaN")
        elif torch.isinf(output).any():
            print(f"   æ•°å€¼æ£€æŸ¥: âŒ è¾“å‡ºåŒ…å«Inf")
        else:
            print(f"   æ•°å€¼æ£€æŸ¥: âœ… é€šè¿‡")
            
    except Exception as e:
        print(f"âŒ æ³¨æ„åŠ›æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    return True


def test_mlp():
    """æµ‹è¯•MLPæ¨¡å—"""
    print("\næµ‹è¯• Qwen3MLP æ¨¡å—...")
    
    # åˆ›å»ºé…ç½®
    config = Qwen3Config(
        hidden_size=512,
        intermediate_size=1024,
    )
    
    # åˆ›å»ºæ¨¡å—
    mlp = Qwen3MLP(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 10
    hidden_size = config.hidden_size
    
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    try:
        output = mlp(hidden_states)
        print(f"âœ… MLPæ¨¡å—æµ‹è¯•é€šè¿‡")
        print(f"   è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        expected_shape = (batch_size, seq_len, hidden_size)
        if output.shape == expected_shape:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âœ… é€šè¿‡")
        else:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âŒ å¤±è´¥ï¼ŒæœŸæœ› {expected_shape}ï¼Œå¾—åˆ° {output.shape}")
            
        # æ£€æŸ¥è¾“å‡ºæ˜¯å¦åŒ…å«NaNæˆ–Inf
        if torch.isnan(output).any():
            print(f"   æ•°å€¼æ£€æŸ¥: âŒ è¾“å‡ºåŒ…å«NaN")
        elif torch.isinf(output).any():
            print(f"   æ•°å€¼æ£€æŸ¥: âŒ è¾“å‡ºåŒ…å«Inf")
        else:
            print(f"   æ•°å€¼æ£€æŸ¥: âœ… é€šè¿‡")
            
    except Exception as e:
        print(f"âŒ MLPæ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    return True


def test_gqa():
    """æµ‹è¯•GQAæœºåˆ¶"""
    print("\næµ‹è¯• GQA æœºåˆ¶...")
    
    # åˆ›å»ºGQAé…ç½®
    config = Qwen3Config(
        hidden_size=512,
        num_attention_heads=8,
        num_key_value_heads=2,  # GQA: 8ä¸ªQå¤´å…±äº«2ä¸ªKVå¤´
        max_position_embeddings=2048,
        rms_norm_eps=1e-6,
        rope_theta=10000.0
    )
    
    # åˆ›å»ºæ¨¡å—
    attention = Qwen3Attention(config)
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    batch_size = 2
    seq_len = 10
    hidden_size = config.hidden_size
    
    hidden_states = torch.randn(batch_size, seq_len, hidden_size)
    
    # å‰å‘ä¼ æ’­
    try:
        output = attention(hidden_states)
        print(f"âœ… GQAæœºåˆ¶æµ‹è¯•é€šè¿‡")
        print(f"   Qå¤´æ•°: {config.num_attention_heads}")
        print(f"   KVå¤´æ•°: {config.num_key_value_heads}")
        print(f"   è¾“å…¥å½¢çŠ¶: {hidden_states.shape}")
        print(f"   è¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶æ˜¯å¦æ­£ç¡®
        expected_shape = (batch_size, seq_len, hidden_size)
        if output.shape == expected_shape:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âœ… é€šè¿‡")
        else:
            print(f"   å½¢çŠ¶æ£€æŸ¥: âŒ å¤±è´¥ï¼ŒæœŸæœ› {expected_shape}ï¼Œå¾—åˆ° {output.shape}")
            
    except Exception as e:
        print(f"âŒ GQAæœºåˆ¶æµ‹è¯•å¤±è´¥: {e}")
        return False
        
    return True


def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("=" * 50)
    print("å¼€å§‹åŸºç¡€æ¨¡å—æµ‹è¯•...")
    print("=" * 50)
    
    all_passed = True
    
    # æµ‹è¯•å„ä¸ªæ¨¡å—
    all_passed &= test_attention()
    all_passed &= test_mlp()
    all_passed &= test_gqa()
    
    print("\n" + "=" * 50)
    if all_passed:
        print("ğŸ‰ æ‰€æœ‰åŸºç¡€æµ‹è¯•é€šè¿‡ï¼")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥å®ç°")
    print("=" * 50)


if __name__ == "__main__":
    main()
