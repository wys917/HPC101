#!/usr/bin/env python3
"""
Qwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜å ç”¨è®¡ç®—è„šæœ¬
"""

import torch
import numpy as np
from modules import Qwen3Config, Qwen3ForCausalLM
import os
import gc

def format_number(num):
    """æ ¼å¼åŒ–æ•°å­—ä¸ºæ˜“è¯»å½¢å¼"""
    if num >= 1e9:
        return f"{num/1e9:.2f}B"
    elif num >= 1e6:
        return f"{num/1e6:.2f}M"
    elif num >= 1e3:
        return f"{num/1e3:.2f}K"
    else:
        return str(num)

def calculate_theoretical_params():
    """ç†è®ºè®¡ç®—å‚æ•°é‡"""
    print("=" * 60)
    print("ğŸ§® ç†è®ºå‚æ•°é‡è®¡ç®—")
    print("=" * 60)
    
    # ä»é…ç½®æ–‡ä»¶è·å–å‚æ•°
    config = Qwen3Config()
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  vocab_size: {config.vocab_size:,}")
    print(f"  hidden_size: {config.hidden_size:,}")
    print(f"  intermediate_size: {config.intermediate_size:,}")
    print(f"  num_hidden_layers: {config.num_hidden_layers}")
    print(f"  num_attention_heads: {config.num_attention_heads}")
    print(f"  num_key_value_heads: {config.num_key_value_heads}")
    print()
    
    # 1. Word Embeddingå±‚å‚æ•°
    embedding_params = config.vocab_size * config.hidden_size
    print(f"1. Word Embeddingå±‚:")
    print(f"   vocab_size Ã— hidden_size = {config.vocab_size:,} Ã— {config.hidden_size:,}")
    print(f"   å‚æ•°é‡: {embedding_params:,} ({format_number(embedding_params)})")
    print()
    
    # 2. æ¯ä¸ªDecoder Layerçš„å‚æ•°é‡
    print(f"2. å•ä¸ªDecoder Layerå‚æ•°é‡:")
    
    # 2.1 Self-Attentionå‚æ•°
    # QKVæŠ•å½±: hidden_size â†’ (Q_dim + K_dim + V_dim)
    q_dim = config.num_attention_heads * (config.hidden_size // config.num_attention_heads)  # 4096
    kv_dim = config.num_key_value_heads * (config.hidden_size // config.num_attention_heads)  # 1024
    qkv_params = config.hidden_size * (q_dim + kv_dim + kv_dim)  # 4096 * (4096 + 1024 + 1024)
    
    # è¾“å‡ºæŠ•å½±: hidden_size â†’ hidden_size
    o_proj_params = config.hidden_size * config.hidden_size
    
    # Q/Kå½’ä¸€åŒ–å±‚å‚æ•°
    qk_norm_params = 2 * config.hidden_size  # q_norm + k_norm
    
    attention_params = qkv_params + o_proj_params + qk_norm_params
    
    print(f"   Self-Attention:")
    print(f"     QKVæŠ•å½±: {config.hidden_size:,} Ã— {q_dim + 2*kv_dim:,} = {qkv_params:,}")
    print(f"     è¾“å‡ºæŠ•å½±: {config.hidden_size:,} Ã— {config.hidden_size:,} = {o_proj_params:,}")
    print(f"     Q/Kå½’ä¸€åŒ–: 2 Ã— {config.hidden_size:,} = {qk_norm_params:,}")
    print(f"     å°è®¡: {attention_params:,} ({format_number(attention_params)})")
    print()
    
    # 2.2 MLP (SwiGLU) å‚æ•°
    gate_params = config.hidden_size * config.intermediate_size
    up_params = config.hidden_size * config.intermediate_size  
    down_params = config.intermediate_size * config.hidden_size
    
    mlp_params = gate_params + up_params + down_params
    
    print(f"   MLP (SwiGLU):")
    print(f"     GateæŠ•å½±: {config.hidden_size:,} Ã— {config.intermediate_size:,} = {gate_params:,}")
    print(f"     UpæŠ•å½±: {config.hidden_size:,} Ã— {config.intermediate_size:,} = {up_params:,}")
    print(f"     DownæŠ•å½±: {config.intermediate_size:,} Ã— {config.hidden_size:,} = {down_params:,}")
    print(f"     å°è®¡: {mlp_params:,} ({format_number(mlp_params)})")
    print()
    
    # 2.3 LayerNormå‚æ•°
    layernorm_params = 2 * config.hidden_size  # input_layernorm + post_attention_layernorm
    
    print(f"   LayerNorm:")
    print(f"     2å±‚ Ã— {config.hidden_size:,} = {layernorm_params:,}")
    print()
    
    # å•å±‚æ€»å‚æ•°
    single_layer_params = attention_params + mlp_params + layernorm_params
    print(f"   å•å±‚æ€»å‚æ•°: {single_layer_params:,} ({format_number(single_layer_params)})")
    print()
    
    # 3. æ‰€æœ‰Decoder Layerså‚æ•°
    all_layers_params = single_layer_params * config.num_hidden_layers
    print(f"3. æ‰€æœ‰{config.num_hidden_layers}å±‚Decoderå‚æ•°:")
    print(f"   {single_layer_params:,} Ã— {config.num_hidden_layers} = {all_layers_params:,} ({format_number(all_layers_params)})")
    print()
    
    # 4. æœ€ç»ˆè¾“å‡ºå±‚å‚æ•° (Language Model Head)
    lm_head_params = config.hidden_size * config.vocab_size
    print(f"4. Language Model Head:")
    print(f"   hidden_size Ã— vocab_size = {config.hidden_size:,} Ã— {config.vocab_size:,}")
    print(f"   å‚æ•°é‡: {lm_head_params:,} ({format_number(lm_head_params)})")
    print()
    
    # 5. Final LayerNorm
    final_norm_params = config.hidden_size
    print(f"5. Final LayerNorm:")
    print(f"   å‚æ•°é‡: {final_norm_params:,}")
    print()
    
    # æ€»å‚æ•°é‡
    total_params = embedding_params + all_layers_params + lm_head_params + final_norm_params
    
    print("=" * 60)
    print("ğŸ“Š å‚æ•°é‡æ±‡æ€»:")
    print("=" * 60)
    print(f"Word Embedding:     {embedding_params:,} ({format_number(embedding_params)})")
    print(f"Decoder Layers:     {all_layers_params:,} ({format_number(all_layers_params)})")
    print(f"LM Head:           {lm_head_params:,} ({format_number(lm_head_params)})")
    print(f"Final LayerNorm:   {final_norm_params:,}")
    print("-" * 60)
    print(f"æ€»å‚æ•°é‡:          {total_params:,} ({format_number(total_params)})")
    print("=" * 60)
    
    return total_params

def calculate_memory_usage(total_params):
    """è®¡ç®—ç†è®ºæ˜¾å­˜å ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨è®¡ç®—")
    print("=" * 60)
    
    # bfloat16: æ¯ä¸ªå‚æ•°2å­—èŠ‚
    bytes_per_param = 2
    
    # æ¨¡å‹å‚æ•°æ˜¾å­˜
    model_memory_bytes = total_params * bytes_per_param
    model_memory_gb = model_memory_bytes / (1024**3)
    
    print(f"æ•°æ®ç±»å‹: bfloat16 (æ¯å‚æ•°{bytes_per_param}å­—èŠ‚)")
    print(f"æ¨¡å‹å‚æ•°æ˜¾å­˜: {total_params:,} Ã— {bytes_per_param} = {model_memory_bytes:,} bytes")
    print(f"æ¨¡å‹å‚æ•°æ˜¾å­˜: {model_memory_gb:.2f} GB")
    print()
    
    # æ¨ç†æ—¶çš„é¢å¤–æ˜¾å­˜å¼€é”€ä¼°ç®—
    print("æ¨ç†æ—¶é¢å¤–æ˜¾å­˜å¼€é”€ä¼°ç®—:")
    
    # æ¿€æ´»å€¼æ˜¾å­˜ (ä¼°ç®—)
    batch_size = 1
    seq_len = 100  # å‡è®¾åºåˆ—é•¿åº¦
    hidden_size = 4096
    
    # æ¯å±‚çš„æ¿€æ´»å€¼
    activation_per_layer = batch_size * seq_len * hidden_size * bytes_per_param
    total_activation = activation_per_layer * 32  # 32å±‚
    activation_gb = total_activation / (1024**3)
    
    print(f"  æ¿€æ´»å€¼æ˜¾å­˜ (ä¼°ç®—): {activation_gb:.2f} GB")
    
    # æ³¨æ„åŠ›çŸ©é˜µæ˜¾å­˜
    attention_matrix = batch_size * 32 * seq_len * seq_len * bytes_per_param  # 32ä¸ªæ³¨æ„åŠ›å¤´
    attention_gb = attention_matrix / (1024**3)
    print(f"  æ³¨æ„åŠ›çŸ©é˜µæ˜¾å­˜: {attention_gb:.2f} GB")
    
    # æ€»æ˜¾å­˜ä¼°ç®—
    inference_total_gb = model_memory_gb + activation_gb + attention_gb + 1.0  # +1GB buffer
    
    print("\n" + "-" * 40)
    print(f"æ¨ç†æ—¶æ€»æ˜¾å­˜ä¼°ç®—: {inference_total_gb:.2f} GB")
    print("=" * 60)
    
    return model_memory_gb, inference_total_gb

def test_actual_model():
    """æµ‹è¯•å®é™…æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜å ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ”¬ å®é™…æ¨¡å‹æµ‹è¯•")
    print("=" * 60)
    
    try:
        # è·å–GPUä¿¡æ¯
        if torch.cuda.is_available():
            print(f"GPUè®¾å¤‡: {torch.cuda.get_device_name()}")
            print(f"GPUæ€»æ˜¾å­˜: {torch.cuda.get_device_properties(0).total_memory / (1024**3):.2f} GB")
        else:
            print("âŒ CUDAä¸å¯ç”¨ï¼Œä½¿ç”¨CPUè¿›è¡Œæµ‹è¯•")
        
        # æ¸…ç†æ˜¾å­˜
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()
        
        # è®°å½•åˆå§‹æ˜¾å­˜
        if torch.cuda.is_available():
            initial_memory = torch.cuda.memory_allocated() / (1024**3)
            print(f"åˆå§‹æ˜¾å­˜å ç”¨: {initial_memory:.2f} GB")
        
        # åˆ›å»ºæ¨¡å‹é…ç½®
        print("\nåˆ›å»ºæ¨¡å‹...")
        config = Qwen3Config()
        
        # åˆ›å»ºæ¨¡å‹ (æš‚æ—¶åœ¨CPUä¸Š)
        model = Qwen3ForCausalLM(config)
        
        # è®¡ç®—å®é™…å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"å®é™…æ€»å‚æ•°é‡: {total_params:,} ({format_number(total_params)})")
        print(f"å¯è®­ç»ƒå‚æ•°é‡: {trainable_params:,} ({format_number(trainable_params)})")
        
        # è¯¦ç»†å‚æ•°åˆ†è§£
        print("\nè¯¦ç»†å‚æ•°åˆ†è§£:")
        for name, param in model.named_parameters():
            if param.numel() > 1000000:  # åªæ˜¾ç¤ºå¤§äº1Må‚æ•°çš„å±‚
                print(f"  {name}: {param.numel():,} ({format_number(param.numel())})")
        
        if torch.cuda.is_available():
            # ç§»åŠ¨åˆ°GPUå¹¶æµ‹è¯•æ˜¾å­˜
            print("\nç§»åŠ¨æ¨¡å‹åˆ°GPU...")
            model = model.cuda()
            model = model.to(torch.bfloat16)  # è½¬æ¢ä¸ºbfloat16
            
            # è®°å½•åŠ è½½åæ˜¾å­˜
            after_load_memory = torch.cuda.memory_allocated() / (1024**3)
            model_memory_actual = after_load_memory - initial_memory
            print(f"æ¨¡å‹åŠ è½½åæ˜¾å­˜: {after_load_memory:.2f} GB")
            print(f"æ¨¡å‹å®é™…å ç”¨æ˜¾å­˜: {model_memory_actual:.2f} GB")
            
            # æ¨ç†æµ‹è¯•
            print("\nè¿›è¡Œæ¨ç†æµ‹è¯•...")
            model.eval()
            
            with torch.no_grad():
                # åˆ›å»ºè¾“å…¥
                batch_size = 1
                seq_len = 100
                input_ids = torch.randint(0, config.vocab_size, (batch_size, seq_len)).cuda()
                
                # å‰å‘ä¼ æ’­
                outputs = model(input_ids)
                
                # è®°å½•æ¨ç†æ—¶æ˜¾å­˜
                inference_memory = torch.cuda.memory_allocated() / (1024**3)
                inference_overhead = inference_memory - after_load_memory
                
                print(f"æ¨ç†æ—¶æ€»æ˜¾å­˜: {inference_memory:.2f} GB")
                print(f"æ¨ç†é¢å¤–å¼€é”€: {inference_overhead:.2f} GB")
                
                return total_params, model_memory_actual, inference_memory
        else:
            return total_params, None, None
        
    except Exception as e:
        print(f"âŒ æ¨¡å‹æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None

def main():
    print("ğŸš€ Qwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜åˆ†æ")
    print("=" * 80)
    
    # 1. ç†è®ºè®¡ç®—
    theoretical_params = calculate_theoretical_params()
    theoretical_model_memory, theoretical_inference_memory = calculate_memory_usage(theoretical_params)
    
    # 2. å®é™…æµ‹è¯•
    actual_params, actual_model_memory, actual_inference_memory = test_actual_model()
    
    # 3. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“Š ç†è®ºvså®é™…å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    if actual_params is not None:
        # å‚æ•°é‡å¯¹æ¯”
        param_diff = abs(theoretical_params - actual_params)
        param_diff_pct = (param_diff / theoretical_params) * 100
        
        print(f"å‚æ•°é‡å¯¹æ¯”:")
        print(f"  ç†è®ºå€¼: {theoretical_params:,} ({format_number(theoretical_params)})")
        print(f"  å®é™…å€¼: {actual_params:,} ({format_number(actual_params)})")
        print(f"  å·®å¼‚:   {param_diff:,} ({param_diff_pct:.2f}%)")
        
        if param_diff_pct < 1:
            print(f"  âœ… å‚æ•°é‡åŸºæœ¬ä¸€è‡´")
        else:
            print(f"  âš ï¸ å‚æ•°é‡å­˜åœ¨å·®å¼‚")
        
        print()
        
        # æ˜¾å­˜å¯¹æ¯”
        if actual_model_memory is not None:
            memory_diff = abs(theoretical_model_memory - actual_model_memory)
            memory_diff_pct = (memory_diff / theoretical_model_memory) * 100
            
            print(f"æ¨¡å‹æ˜¾å­˜å¯¹æ¯”:")
            print(f"  ç†è®ºå€¼: {theoretical_model_memory:.2f} GB")
            print(f"  å®é™…å€¼: {actual_model_memory:.2f} GB")
            print(f"  å·®å¼‚:   {memory_diff:.2f} GB ({memory_diff_pct:.2f}%)")
            
            if memory_diff_pct < 10:
                print(f"  âœ… æ˜¾å­˜å ç”¨åŸºæœ¬ä¸€è‡´")
            else:
                print(f"  âš ï¸ æ˜¾å­˜å ç”¨å­˜åœ¨å·®å¼‚")
        
        print()
        
        # æ¨ç†æ˜¾å­˜åˆ†æ
        if actual_inference_memory is not None:
            print(f"æ¨ç†æ˜¾å­˜åˆ†æ:")
            print(f"  ç†è®ºæ¨ç†æ˜¾å­˜: {theoretical_inference_memory:.2f} GB")
            print(f"  å®é™…æ¨ç†æ˜¾å­˜: {actual_inference_memory:.2f} GB")
            inference_diff = abs(theoretical_inference_memory - actual_inference_memory)
            print(f"  å·®å¼‚: {inference_diff:.2f} GB")
    
    # 4. å·®å¼‚åŸå› åˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ” å·®å¼‚åŸå› åˆ†æ")
    print("=" * 60)
    print("å¯èƒ½çš„å·®å¼‚åŸå› :")
    print("1. å†…å­˜å¯¹é½: GPUè¦æ±‚å†…å­˜æŒ‰ç‰¹å®šè¾¹ç•Œå¯¹é½ï¼Œå¯èƒ½å¯¼è‡´é¢å¤–å¼€é”€")
    print("2. PyTorchå¼€é”€: æ¡†æ¶æœ¬èº«çš„å…ƒæ•°æ®å’Œç®¡ç†å¼€é”€")
    print("3. CUDAä¸Šä¸‹æ–‡: GPUé©±åŠ¨å’ŒCUDAè¿è¡Œæ—¶çš„åŸºç¡€å¼€é”€")
    print("4. ç¼“å­˜å¼€é”€: PyTorchçš„å†…å­˜æ± å’Œç¼“å­˜æœºåˆ¶")
    print("5. ç²¾åº¦å·®å¼‚: ç†è®ºè®¡ç®—å¯èƒ½é—æ¼æŸäº›å°çš„å‚æ•°ç»„ä»¶")
    print("6. åŠ¨æ€å†…å­˜: æ¨ç†è¿‡ç¨‹ä¸­ä¸´æ—¶å¼ é‡çš„å†…å­˜åˆ†é…")
    print()
    
    print("âœ… åˆ†æå®Œæˆ!")

if __name__ == "__main__":
    main()
