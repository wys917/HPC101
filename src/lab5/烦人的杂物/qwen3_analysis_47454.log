ğŸš€ å¼€å§‹Qwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜åˆ†æ
ä½œä¸šID: 47454
èŠ‚ç‚¹: v20
æ—¶é—´: Mon Aug 25 10:31:24 PM CST 2025
================================
GPUä¿¡æ¯:
Mon Aug 25 22:31:24 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:19:00.0 Off |                    0 |
| N/A   39C    P0             43W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+
================================
è¿è¡Œå‚æ•°é‡å’Œæ˜¾å­˜åˆ†æ...
ğŸš€ Qwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜åˆ†æ
================================================================================
============================================================
ğŸ§® ç†è®ºå‚æ•°é‡è®¡ç®—
============================================================
æ¨¡å‹é…ç½®:
  vocab_size: 152,064
  hidden_size: 4,096
  intermediate_size: 11,008
  num_hidden_layers: 32
  num_attention_heads: 32
  num_key_value_heads: 32

1. Word Embeddingå±‚:
   vocab_size Ã— hidden_size = 152,064 Ã— 4,096
   å‚æ•°é‡: 622,854,144 (622.85M)

2. å•ä¸ªDecoder Layerå‚æ•°é‡:
   Self-Attention:
     QKVæŠ•å½±: 4,096 Ã— 12,288 = 50,331,648
     è¾“å‡ºæŠ•å½±: 4,096 Ã— 4,096 = 16,777,216
     Q/Kå½’ä¸€åŒ–: 2 Ã— 4,096 = 8,192
     å°è®¡: 67,117,056 (67.12M)

   MLP (SwiGLU):
     GateæŠ•å½±: 4,096 Ã— 11,008 = 45,088,768
     UpæŠ•å½±: 4,096 Ã— 11,008 = 45,088,768
     DownæŠ•å½±: 11,008 Ã— 4,096 = 45,088,768
     å°è®¡: 135,266,304 (135.27M)

   LayerNorm:
     2å±‚ Ã— 4,096 = 8,192

   å•å±‚æ€»å‚æ•°: 202,391,552 (202.39M)

3. æ‰€æœ‰32å±‚Decoderå‚æ•°:
   202,391,552 Ã— 32 = 6,476,529,664 (6.48B)

4. Language Model Head:
   hidden_size Ã— vocab_size = 4,096 Ã— 152,064
   å‚æ•°é‡: 622,854,144 (622.85M)

5. Final LayerNorm:
   å‚æ•°é‡: 4,096

============================================================
ğŸ“Š å‚æ•°é‡æ±‡æ€»:
============================================================
Word Embedding:     622,854,144 (622.85M)
Decoder Layers:     6,476,529,664 (6.48B)
LM Head:           622,854,144 (622.85M)
Final LayerNorm:   4,096
------------------------------------------------------------
æ€»å‚æ•°é‡:          7,722,242,048 (7.72B)
============================================================

============================================================
ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨è®¡ç®—
============================================================
æ•°æ®ç±»å‹: bfloat16 (æ¯å‚æ•°2å­—èŠ‚)
æ¨¡å‹å‚æ•°æ˜¾å­˜: 7,722,242,048 Ã— 2 = 15,444,484,096 bytes
æ¨¡å‹å‚æ•°æ˜¾å­˜: 14.38 GB

æ¨ç†æ—¶é¢å¤–æ˜¾å­˜å¼€é”€ä¼°ç®—:
  æ¿€æ´»å€¼æ˜¾å­˜ (ä¼°ç®—): 0.02 GB
  æ³¨æ„åŠ›çŸ©é˜µæ˜¾å­˜: 0.00 GB

----------------------------------------
æ¨ç†æ—¶æ€»æ˜¾å­˜ä¼°ç®—: 15.41 GB
============================================================

============================================================
ğŸ”¬ å®é™…æ¨¡å‹æµ‹è¯•
============================================================
GPUè®¾å¤‡: Tesla V100-SXM2-32GB
GPUæ€»æ˜¾å­˜: 31.73 GB
åˆå§‹æ˜¾å­˜å ç”¨: 0.00 GB

åˆ›å»ºæ¨¡å‹...
å®é™…æ€»å‚æ•°é‡: 7,721,988,096 (7.72B)
å¯è®­ç»ƒå‚æ•°é‡: 7,721,988,096 (7.72B)

è¯¦ç»†å‚æ•°åˆ†è§£:
  model.embed_tokens.weight: 622,854,144 (622.85M)
  model.layers.0.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.0.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.0.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.0.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.0.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.0.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.0.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.1.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.1.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.1.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.1.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.1.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.1.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.1.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.2.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.2.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.2.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.2.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.2.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.2.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.2.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.3.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.3.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.3.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.3.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.3.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.3.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.3.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.4.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.4.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.4.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.4.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.4.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.4.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.4.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.5.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.5.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.5.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.5.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.5.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.5.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.5.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.6.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.6.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.6.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.6.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.6.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.6.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.6.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.7.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.7.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.7.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.7.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.7.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.7.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.7.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.8.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.8.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.8.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.8.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.8.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.8.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.8.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.9.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.9.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.9.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.9.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.9.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.9.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.9.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.10.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.10.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.10.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.10.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.10.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.10.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.10.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.11.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.11.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.11.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.11.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.11.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.11.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.11.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.12.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.12.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.12.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.12.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.12.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.12.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.12.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.13.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.13.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.13.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.13.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.13.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.13.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.13.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.14.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.14.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.14.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.14.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.14.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.14.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.14.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.15.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.15.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.15.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.15.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.15.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.15.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.15.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.16.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.16.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.16.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.16.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.16.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.16.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.16.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.17.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.17.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.17.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.17.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.17.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.17.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.17.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.18.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.18.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.18.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.18.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.18.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.18.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.18.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.19.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.19.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.19.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.19.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.19.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.19.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.19.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.20.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.20.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.20.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.20.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.20.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.20.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.20.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.21.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.21.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.21.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.21.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.21.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.21.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.21.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.22.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.22.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.22.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.22.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.22.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.22.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.22.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.23.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.23.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.23.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.23.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.23.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.23.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.23.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.24.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.24.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.24.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.24.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.24.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.24.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.24.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.25.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.25.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.25.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.25.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.25.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.25.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.25.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.26.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.26.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.26.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.26.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.26.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.26.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.26.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.27.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.27.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.27.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.27.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.27.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.27.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.27.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.28.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.28.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.28.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.28.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.28.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.28.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.28.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.29.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.29.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.29.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.29.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.29.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.29.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.29.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.30.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.30.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.30.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.30.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.30.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.30.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.30.mlp.down_proj.weight: 45,088,768 (45.09M)
  model.layers.31.self_attn.q_proj.weight: 16,777,216 (16.78M)
  model.layers.31.self_attn.k_proj.weight: 16,777,216 (16.78M)
  model.layers.31.self_attn.v_proj.weight: 16,777,216 (16.78M)
  model.layers.31.self_attn.o_proj.weight: 16,777,216 (16.78M)
  model.layers.31.mlp.gate_proj.weight: 45,088,768 (45.09M)
  model.layers.31.mlp.up_proj.weight: 45,088,768 (45.09M)
  model.layers.31.mlp.down_proj.weight: 45,088,768 (45.09M)
  lm_head.weight: 622,854,144 (622.85M)

ç§»åŠ¨æ¨¡å‹åˆ°GPU...
æ¨¡å‹åŠ è½½åæ˜¾å­˜: 14.88 GB
æ¨¡å‹å®é™…å ç”¨æ˜¾å­˜: 14.88 GB

è¿›è¡Œæ¨ç†æµ‹è¯•...
æ¨ç†æ—¶æ€»æ˜¾å­˜: 14.95 GB
æ¨ç†é¢å¤–å¼€é”€: 0.06 GB

============================================================
ğŸ“Š ç†è®ºvså®é™…å¯¹æ¯”åˆ†æ
============================================================
å‚æ•°é‡å¯¹æ¯”:
  ç†è®ºå€¼: 7,722,242,048 (7.72B)
  å®é™…å€¼: 7,721,988,096 (7.72B)
  å·®å¼‚:   253,952 (0.00%)
  âœ… å‚æ•°é‡åŸºæœ¬ä¸€è‡´

æ¨¡å‹æ˜¾å­˜å¯¹æ¯”:
  ç†è®ºå€¼: 14.38 GB
  å®é™…å€¼: 14.88 GB
  å·®å¼‚:   0.50 GB (3.47%)
  âœ… æ˜¾å­˜å ç”¨åŸºæœ¬ä¸€è‡´

æ¨ç†æ˜¾å­˜åˆ†æ:
  ç†è®ºæ¨ç†æ˜¾å­˜: 15.41 GB
  å®é™…æ¨ç†æ˜¾å­˜: 14.95 GB
  å·®å¼‚: 0.46 GB

============================================================
ğŸ” å·®å¼‚åŸå› åˆ†æ
============================================================
å¯èƒ½çš„å·®å¼‚åŸå› :
1. å†…å­˜å¯¹é½: GPUè¦æ±‚å†…å­˜æŒ‰ç‰¹å®šè¾¹ç•Œå¯¹é½ï¼Œå¯èƒ½å¯¼è‡´é¢å¤–å¼€é”€
2. PyTorchå¼€é”€: æ¡†æ¶æœ¬èº«çš„å…ƒæ•°æ®å’Œç®¡ç†å¼€é”€
3. CUDAä¸Šä¸‹æ–‡: GPUé©±åŠ¨å’ŒCUDAè¿è¡Œæ—¶çš„åŸºç¡€å¼€é”€
4. ç¼“å­˜å¼€é”€: PyTorchçš„å†…å­˜æ± å’Œç¼“å­˜æœºåˆ¶
5. ç²¾åº¦å·®å¼‚: ç†è®ºè®¡ç®—å¯èƒ½é—æ¼æŸäº›å°çš„å‚æ•°ç»„ä»¶
6. åŠ¨æ€å†…å­˜: æ¨ç†è¿‡ç¨‹ä¸­ä¸´æ—¶å¼ é‡çš„å†…å­˜åˆ†é…

âœ… åˆ†æå®Œæˆ!
================================
åˆ†æå®Œæˆï¼Œæ—¶é—´: Mon Aug 25 10:32:47 PM CST 2025
å¼€å§‹åˆ†æQwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜å ç”¨...
æ—¶é—´: Mon Aug 25 10:32:47 PM CST 2025
èŠ‚ç‚¹: v20
GPU: 0
Pythonç¯å¢ƒä¿¡æ¯:
Python 3.11.2
PyTorchç‰ˆæœ¬: 2.6.0+cu124
CUDAå¯ç”¨: True

å¼€å§‹æ˜¾å­˜åŸºçº¿æµ‹é‡...
Mon Aug 25 22:32:53 2025       
+-----------------------------------------------------------------------------------------+
| NVIDIA-SMI 575.57.08              Driver Version: 575.57.08      CUDA Version: 12.9     |
|-----------------------------------------+------------------------+----------------------+
| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |
| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |
|                                         |                        |               MIG M. |
|=========================================+========================+======================|
|   0  Tesla V100-SXM2-32GB           On  |   00000000:19:00.0 Off |                    0 |
| N/A   41C    P0             44W /  300W |       0MiB /  32768MiB |      0%      Default |
|                                         |                        |                  N/A |
+-----------------------------------------+------------------------+----------------------+
                                                                                         
+-----------------------------------------------------------------------------------------+
| Processes:                                                                              |
|  GPU   GI   CI              PID   Type   Process name                        GPU Memory |
|        ID   ID                                                               Usage      |
|=========================================================================================|
|  No running processes found                                                             |
+-----------------------------------------------------------------------------------------+

è¿è¡Œæ¨¡å‹åˆ†æè„šæœ¬...
Qwen3-8B æ¨¡å‹åˆ†æå·¥å…·
åˆ†æåŒ…æ‹¬ï¼šå‚æ•°é‡è®¡ç®—ã€æ˜¾å­˜å ç”¨ä¼°ç®—ã€å®é™…ä½¿ç”¨æƒ…å†µå¯¹æ¯”
============================================================
ğŸ§® Qwen3-8B æ¨¡å‹ç†è®ºå‚æ•°é‡è®¡ç®—
============================================================
æ¨¡å‹é…ç½®:
  è¯æ±‡è¡¨å¤§å°: 152,064
  éšè—å±‚ç»´åº¦: 4,096
  FFNä¸­é—´ç»´åº¦: 11,008
  å±‚æ•°: 32
  æ³¨æ„åŠ›å¤´æ•°: 32
  KVå¤´æ•°: 32

ğŸ“ 1. Embeddingå±‚å‚æ•°è®¡ç®—:
  Word Embedding: 152,064 Ã— 4,096 = 622,854,144

ğŸ—ï¸ 2. æ¯ä¸ªDecoder Layerå‚æ•°è®¡ç®—:
  2.1 Self-Attention:
    QKVæŠ•å½±: 4,096 Ã— 12,288 = 50,331,648
    è¾“å‡ºæŠ•å½±: 4,096 Ã— 4,096 = 16,777,216
    Q/Kå½’ä¸€åŒ–: 2 Ã— 32 Ã— 128 = 8,192
    Self-Attentionæ€»è®¡: 67,117,056
  2.2 MLP (SwiGLU):
    GateæŠ•å½±: 4,096 Ã— 11,008 = 45,088,768
    UpæŠ•å½±: 4,096 Ã— 11,008 = 45,088,768
    DownæŠ•å½±: 11,008 Ã— 4,096 = 45,088,768
    MLPæ€»è®¡: 135,266,304
  2.3 LayerNorm:
    LayerNorm Ã— 2: 2 Ã— 4,096 = 8,192
  å•å±‚æ€»è®¡: 202,391,552

ğŸ”¢ 3. æ‰€æœ‰å±‚å‚æ•°è®¡ç®—:
  32å±‚æ€»è®¡: 32 Ã— 202,391,552 = 6,476,529,664

ğŸ“¤ 4. æœ€ç»ˆè¾“å‡ºå±‚å‚æ•°:
  Final LayerNorm: 4,096
  LM Head: 4,096 Ã— 152,064 = 622,854,144
  è¾“å‡ºå±‚æ€»è®¡: 622,858,240

ğŸ¯ 5. æ€»å‚æ•°é‡:
  Embedding: 622,854,144
  Decoder Layers: 6,476,529,664
  è¾“å‡ºå±‚: 622,858,240
  æ€»è®¡: 7,722,242,048
  çº¦ 7.72B å‚æ•°

============================================================
ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨è®¡ç®— (bfloat16)
============================================================
æ¨¡å‹å‚æ•°æ˜¾å­˜:
  å‚æ•°é‡: 7,722,242,048
  æ•°æ®ç±»å‹: bfloat16 (2 bytes/param)
  å‚æ•°æ˜¾å­˜: 7,722,242,048 Ã— 2 = 15,444,484,096 bytes
  å‚æ•°æ˜¾å­˜: 14.38 GB

æ¿€æ´»æ˜¾å­˜ä¼°ç®— (batch=1, seq_len=1024):
  Hidden States: 0.25 GB
  Attention Weights: 2.00 GB
  KV Cache: 0.50 GB
  æ¿€æ´»æ€»è®¡: 2.75 GB

æ€»æ˜¾å­˜éœ€æ±‚:
  æ¨¡å‹å‚æ•°: 14.38 GB
  æ¿€æ´»å†…å­˜: 2.75 GB
  ç†è®ºæ€»è®¡: 17.13 GB

============================================================
ğŸ” å®é™…æ¨¡å‹å‚æ•°é‡éªŒè¯
============================================================
å®é™…æ¨¡å‹å‚æ•°ç»Ÿè®¡:
  æ€»å‚æ•°é‡: 7,721,988,096
  å¯è®­ç»ƒå‚æ•°: 7,721,988,096
  çº¦ 7.72B å‚æ•°

æŒ‰æ¨¡å—å‚æ•°ç»Ÿè®¡:
  model: 7,099,133,952 (91.9%)
  lm_head: 622,854,144 (8.1%)

============================================================
ğŸ“Š å®é™…æ˜¾å­˜å ç”¨æŸ¥è¯¢
============================================================
GPU 0: Tesla V100-SXM2-32GB
  æ€»æ˜¾å­˜: 32,768 MB (32.00 GB)
  å·²ç”¨æ˜¾å­˜: 0 MB (0.00 GB)
  å¯ç”¨æ˜¾å­˜: 32,495 MB (31.73 GB)
  ä½¿ç”¨ç‡: 0.0%

PyTorchæ˜¾å­˜ä¿¡æ¯:
  GPU 0:
    æ€»æ˜¾å­˜: 31.73 GB
    å·²åˆ†é…: 0.00 GB
    å·²ç¼“å­˜: 0.00 GB

============================================================
ğŸ“ˆ å¯¹æ¯”åˆ†æ
============================================================
å‚æ•°é‡å¯¹æ¯”:
  ç†è®ºè®¡ç®—: 7,722,242,048
  å®é™…ç»Ÿè®¡: 7,721,988,096
  å·®å¼‚: 253,952 (0.00%)
  âœ… ç†è®ºè®¡ç®—ä¸å®é™…åŸºæœ¬ä¸€è‡´

åˆ†æå®Œæˆ!
ç»“æŸæ—¶é—´: Mon Aug 25 10:33:59 PM CST 2025
