#!/usr/bin/env python3
"""
Qwen3æ¨¡å‹å‚æ•°é‡å’Œæ˜¾å­˜å ç”¨è®¡ç®—è„šæœ¬

æœ¬è„šæœ¬ç”¨äºï¼š
1. ç†è®ºè®¡ç®—Qwen3æ¨¡å‹çš„å‚æ•°é‡
2. è®¡ç®—ç†è®ºæ˜¾å­˜å ç”¨
3. æŸ¥çœ‹å®é™…æ˜¾å­˜å ç”¨
4. å¯¹æ¯”åˆ†æå·®å¼‚
"""

import torch
import subprocess
import os
from modules import Qwen3Config, Qwen3ForCausalLM


def calculate_theoretical_params():
    """è®¡ç®—ç†è®ºå‚æ•°é‡"""
    config = Qwen3Config()
    
    print("=" * 60)
    print("ğŸ§® Qwen3-8B æ¨¡å‹ç†è®ºå‚æ•°é‡è®¡ç®—")
    print("=" * 60)
    
    # åŸºç¡€é…ç½®
    vocab_size = config.vocab_size           # 152,064
    hidden_size = config.hidden_size         # 4,096
    intermediate_size = config.intermediate_size  # 11,008
    num_layers = config.num_hidden_layers    # 32
    num_heads = config.num_attention_heads   # 32
    num_kv_heads = config.num_key_value_heads # 32
    head_dim = hidden_size // num_heads      # 128
    
    print(f"æ¨¡å‹é…ç½®:")
    print(f"  è¯æ±‡è¡¨å¤§å°: {vocab_size:,}")
    print(f"  éšè—å±‚ç»´åº¦: {hidden_size:,}")
    print(f"  FFNä¸­é—´ç»´åº¦: {intermediate_size:,}")
    print(f"  å±‚æ•°: {num_layers}")
    print(f"  æ³¨æ„åŠ›å¤´æ•°: {num_heads}")
    print(f"  KVå¤´æ•°: {num_kv_heads}")
    print()
    
    # 1. Embeddingå±‚å‚æ•°
    print("ğŸ“ 1. Embeddingå±‚å‚æ•°è®¡ç®—:")
    embedding_params = vocab_size * hidden_size
    print(f"  Word Embedding: {vocab_size:,} Ã— {hidden_size:,} = {embedding_params:,}")
    
    # 2. æ¯ä¸ªDecoder Layerçš„å‚æ•°
    print("\nğŸ—ï¸ 2. æ¯ä¸ªDecoder Layerå‚æ•°è®¡ç®—:")
    
    # 2.1 Self-Attentionå‚æ•°
    print("  2.1 Self-Attention:")
    # QKVæŠ•å½±: hidden_size â†’ (Q_dim + K_dim + V_dim)
    q_dim = num_heads * head_dim      # 32 * 128 = 4096
    k_dim = num_kv_heads * head_dim   # 32 * 128 = 4096 (æ ‡å‡†MHAï¼Œä¸æ˜¯GQA)
    v_dim = num_kv_heads * head_dim   # 32 * 128 = 4096
    qkv_dim = q_dim + k_dim + v_dim   # 4096 + 4096 + 4096 = 12288
    
    qkv_proj_params = hidden_size * qkv_dim
    print(f"    QKVæŠ•å½±: {hidden_size:,} Ã— {qkv_dim:,} = {qkv_proj_params:,}")
    
    # è¾“å‡ºæŠ•å½±: hidden_size â†’ hidden_size
    o_proj_params = hidden_size * hidden_size
    print(f"    è¾“å‡ºæŠ•å½±: {hidden_size:,} Ã— {hidden_size:,} = {o_proj_params:,}")
    
    # Q/Kå½’ä¸€åŒ–å±‚ (RMSNorm)
    qk_norm_params = 2 * num_heads * head_dim  # Q_norm + K_norm
    print(f"    Q/Kå½’ä¸€åŒ–: 2 Ã— {num_heads} Ã— {head_dim} = {qk_norm_params:,}")
    
    attention_params = qkv_proj_params + o_proj_params + qk_norm_params
    print(f"    Self-Attentionæ€»è®¡: {attention_params:,}")
    
    # 2.2 MLPå‚æ•°
    print("  2.2 MLP (SwiGLU):")
    # GateæŠ•å½±: hidden_size â†’ intermediate_size
    gate_proj_params = hidden_size * intermediate_size
    print(f"    GateæŠ•å½±: {hidden_size:,} Ã— {intermediate_size:,} = {gate_proj_params:,}")
    
    # UpæŠ•å½±: hidden_size â†’ intermediate_size  
    up_proj_params = hidden_size * intermediate_size
    print(f"    UpæŠ•å½±: {hidden_size:,} Ã— {intermediate_size:,} = {up_proj_params:,}")
    
    # DownæŠ•å½±: intermediate_size â†’ hidden_size
    down_proj_params = intermediate_size * hidden_size
    print(f"    DownæŠ•å½±: {intermediate_size:,} Ã— {hidden_size:,} = {down_proj_params:,}")
    
    mlp_params = gate_proj_params + up_proj_params + down_proj_params
    print(f"    MLPæ€»è®¡: {mlp_params:,}")
    
    # 2.3 LayerNormå‚æ•°
    print("  2.3 LayerNorm:")
    layernorm_params = 2 * hidden_size  # input_layernorm + post_attention_layernorm
    print(f"    LayerNorm Ã— 2: 2 Ã— {hidden_size:,} = {layernorm_params:,}")
    
    # æ¯å±‚æ€»å‚æ•°
    layer_params = attention_params + mlp_params + layernorm_params
    print(f"  å•å±‚æ€»è®¡: {layer_params:,}")
    
    # 3. æ‰€æœ‰å±‚çš„å‚æ•°
    print(f"\nğŸ”¢ 3. æ‰€æœ‰å±‚å‚æ•°è®¡ç®—:")
    total_layer_params = num_layers * layer_params
    print(f"  {num_layers}å±‚æ€»è®¡: {num_layers} Ã— {layer_params:,} = {total_layer_params:,}")
    
    # 4. æœ€ç»ˆè¾“å‡ºå±‚å‚æ•°
    print(f"\nğŸ“¤ 4. æœ€ç»ˆè¾“å‡ºå±‚å‚æ•°:")
    # Final LayerNorm
    final_norm_params = hidden_size
    print(f"  Final LayerNorm: {final_norm_params:,}")
    
    # LM Head (è¯­è¨€æ¨¡å‹å¤´)
    if config.tie_word_embeddings:
        lm_head_params = 0  # å…±äº«embeddingæƒé‡
        print(f"  LM Head: å…±äº«Embeddingæƒé‡, 0å‚æ•°")
    else:
        lm_head_params = hidden_size * vocab_size
        print(f"  LM Head: {hidden_size:,} Ã— {vocab_size:,} = {lm_head_params:,}")
    
    output_params = final_norm_params + lm_head_params
    print(f"  è¾“å‡ºå±‚æ€»è®¡: {output_params:,}")
    
    # 5. æ€»å‚æ•°é‡
    print(f"\nğŸ¯ 5. æ€»å‚æ•°é‡:")
    total_params = embedding_params + total_layer_params + output_params
    print(f"  Embedding: {embedding_params:,}")
    print(f"  Decoder Layers: {total_layer_params:,}")
    print(f"  è¾“å‡ºå±‚: {output_params:,}")
    print(f"  æ€»è®¡: {total_params:,}")
    print(f"  çº¦ {total_params / 1e9:.2f}B å‚æ•°")
    
    return total_params


def calculate_theoretical_memory(total_params):
    """è®¡ç®—ç†è®ºæ˜¾å­˜å ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ’¾ ç†è®ºæ˜¾å­˜å ç”¨è®¡ç®— (bfloat16)")
    print("=" * 60)
    
    # bfloat16: æ¯ä¸ªå‚æ•°2å­—èŠ‚
    bytes_per_param = 2
    
    # æ¨¡å‹å‚æ•°æ˜¾å­˜
    model_memory_bytes = total_params * bytes_per_param
    model_memory_gb = model_memory_bytes / (1024**3)
    
    print(f"æ¨¡å‹å‚æ•°æ˜¾å­˜:")
    print(f"  å‚æ•°é‡: {total_params:,}")
    print(f"  æ•°æ®ç±»å‹: bfloat16 (2 bytes/param)")
    print(f"  å‚æ•°æ˜¾å­˜: {total_params:,} Ã— 2 = {model_memory_bytes:,} bytes")
    print(f"  å‚æ•°æ˜¾å­˜: {model_memory_gb:.2f} GB")
    
    # æ¿€æ´»æ˜¾å­˜ä¼°ç®— (æ¨ç†æ—¶)
    # å‡è®¾batch_size=1, seq_len=1024
    batch_size = 1
    seq_len = 1024
    hidden_size = 4096
    
    # ä¸»è¦æ¿€æ´»æ˜¾å­˜æ¥æº
    print(f"\næ¿€æ´»æ˜¾å­˜ä¼°ç®— (batch={batch_size}, seq_len={seq_len}):")
    
    # Hidden states
    hidden_memory = batch_size * seq_len * hidden_size * bytes_per_param * 32  # 32å±‚
    hidden_memory_gb = hidden_memory / (1024**3)
    print(f"  Hidden States: {hidden_memory_gb:.2f} GB")
    
    # Attention weights
    attention_memory = batch_size * 32 * seq_len * seq_len * bytes_per_param * 32  # 32å¤´Ã—32å±‚
    attention_memory_gb = attention_memory / (1024**3)
    print(f"  Attention Weights: {attention_memory_gb:.2f} GB")
    
    # KV Cache (æ¨ç†æ—¶)
    kv_cache_memory = batch_size * seq_len * hidden_size * bytes_per_param * 2 * 32  # K+VÃ—32å±‚
    kv_cache_memory_gb = kv_cache_memory / (1024**3)
    print(f"  KV Cache: {kv_cache_memory_gb:.2f} GB")
    
    total_activation_gb = hidden_memory_gb + attention_memory_gb + kv_cache_memory_gb
    print(f"  æ¿€æ´»æ€»è®¡: {total_activation_gb:.2f} GB")
    
    # æ€»æ˜¾å­˜éœ€æ±‚
    total_memory_gb = model_memory_gb + total_activation_gb
    print(f"\næ€»æ˜¾å­˜éœ€æ±‚:")
    print(f"  æ¨¡å‹å‚æ•°: {model_memory_gb:.2f} GB")
    print(f"  æ¿€æ´»å†…å­˜: {total_activation_gb:.2f} GB") 
    print(f"  ç†è®ºæ€»è®¡: {total_memory_gb:.2f} GB")
    
    return model_memory_gb, total_memory_gb


def get_actual_memory_usage():
    """è·å–å®é™…æ˜¾å­˜å ç”¨"""
    print("\n" + "=" * 60)
    print("ğŸ“Š å®é™…æ˜¾å­˜å ç”¨æŸ¥è¯¢")
    print("=" * 60)
    
    try:
        # æŸ¥çœ‹GPUä¿¡æ¯
        result = subprocess.run(['nvidia-smi', '--query-gpu=name,memory.total,memory.used,memory.free', 
                               '--format=csv,noheader,nounits'], 
                              capture_output=True, text=True)
        
        if result.returncode == 0:
            lines = result.stdout.strip().split('\n')
            for i, line in enumerate(lines):
                parts = line.split(', ')
                if len(parts) >= 4:
                    gpu_name = parts[0]
                    total_mem = int(parts[1])
                    used_mem = int(parts[2])
                    free_mem = int(parts[3])
                    
                    print(f"GPU {i}: {gpu_name}")
                    print(f"  æ€»æ˜¾å­˜: {total_mem:,} MB ({total_mem/1024:.2f} GB)")
                    print(f"  å·²ç”¨æ˜¾å­˜: {used_mem:,} MB ({used_mem/1024:.2f} GB)")
                    print(f"  å¯ç”¨æ˜¾å­˜: {free_mem:,} MB ({free_mem/1024:.2f} GB)")
                    print(f"  ä½¿ç”¨ç‡: {used_mem/total_mem*100:.1f}%")
                    
        else:
            print("æ— æ³•è·å–GPUä¿¡æ¯ï¼Œå¯èƒ½æœªå®‰è£…nvidia-smiæˆ–æ— å¯ç”¨GPU")
            
    except FileNotFoundError:
        print("nvidia-smiå‘½ä»¤æœªæ‰¾åˆ°ï¼Œå¯èƒ½æœªå®‰è£…NVIDIAé©±åŠ¨")
    
    # ä½¿ç”¨PyTorchæŸ¥çœ‹æ˜¾å­˜
    if torch.cuda.is_available():
        print(f"\nPyTorchæ˜¾å­˜ä¿¡æ¯:")
        for i in range(torch.cuda.device_count()):
            total_mem = torch.cuda.get_device_properties(i).total_memory / (1024**3)
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            cached = torch.cuda.memory_reserved(i) / (1024**3)
            
            print(f"  GPU {i}:")
            print(f"    æ€»æ˜¾å­˜: {total_mem:.2f} GB")
            print(f"    å·²åˆ†é…: {allocated:.2f} GB") 
            print(f"    å·²ç¼“å­˜: {cached:.2f} GB")
    else:
        print("PyTorchæœªæ£€æµ‹åˆ°å¯ç”¨çš„CUDAè®¾å¤‡")


def verify_with_actual_model():
    """ç”¨å®é™…æ¨¡å‹éªŒè¯å‚æ•°é‡"""
    print("\n" + "=" * 60)
    print("ğŸ” å®é™…æ¨¡å‹å‚æ•°é‡éªŒè¯")
    print("=" * 60)
    
    try:
        # åˆ›å»ºæ¨¡å‹ä½†ä¸åŠ è½½æƒé‡
        config = Qwen3Config()
        model = Qwen3ForCausalLM(config)
        
        # è®¡ç®—å®é™…å‚æ•°é‡
        total_params = sum(p.numel() for p in model.parameters())
        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
        
        print(f"å®é™…æ¨¡å‹å‚æ•°ç»Ÿè®¡:")
        print(f"  æ€»å‚æ•°é‡: {total_params:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°: {trainable_params:,}")
        print(f"  çº¦ {total_params / 1e9:.2f}B å‚æ•°")
        
        # æŒ‰æ¨¡å—ç»Ÿè®¡
        print(f"\næŒ‰æ¨¡å—å‚æ•°ç»Ÿè®¡:")
        for name, module in model.named_children():
            module_params = sum(p.numel() for p in module.parameters())
            print(f"  {name}: {module_params:,} ({module_params/total_params*100:.1f}%)")
            
        return total_params
        
    except Exception as e:
        print(f"åˆ›å»ºæ¨¡å‹æ—¶å‡ºé”™: {e}")
        return None


def main():
    """ä¸»å‡½æ•°"""
    print("Qwen3-8B æ¨¡å‹åˆ†æå·¥å…·")
    print("åˆ†æåŒ…æ‹¬ï¼šå‚æ•°é‡è®¡ç®—ã€æ˜¾å­˜å ç”¨ä¼°ç®—ã€å®é™…ä½¿ç”¨æƒ…å†µå¯¹æ¯”")
    
    # 1. ç†è®ºè®¡ç®—
    theoretical_params = calculate_theoretical_params()
    
    # 2. æ˜¾å­˜å ç”¨è®¡ç®—  
    model_memory_gb, total_memory_gb = calculate_theoretical_memory(theoretical_params)
    
    # 3. å®é™…å‚æ•°é‡éªŒè¯
    actual_params = verify_with_actual_model()
    
    # 4. å®é™…æ˜¾å­˜ä½¿ç”¨æƒ…å†µ
    get_actual_memory_usage()
    
    # 5. å¯¹æ¯”åˆ†æ
    print("\n" + "=" * 60)
    print("ğŸ“ˆ å¯¹æ¯”åˆ†æ")
    print("=" * 60)
    
    if actual_params:
        diff = abs(theoretical_params - actual_params)
        diff_percent = diff / theoretical_params * 100
        print(f"å‚æ•°é‡å¯¹æ¯”:")
        print(f"  ç†è®ºè®¡ç®—: {theoretical_params:,}")
        print(f"  å®é™…ç»Ÿè®¡: {actual_params:,}")
        print(f"  å·®å¼‚: {diff:,} ({diff_percent:.2f}%)")
        
        if diff_percent < 1:
            print("  âœ… ç†è®ºè®¡ç®—ä¸å®é™…åŸºæœ¬ä¸€è‡´")
        else:
            print("  â“ ç†è®ºè®¡ç®—ä¸å®é™…å­˜åœ¨å·®å¼‚ï¼Œå¯èƒ½åŸå› :")
            print("     - æŸäº›æ¨¡å—çš„å‚æ•°è®¡ç®—é—æ¼")
            print("     - é…ç½®å‚æ•°ä¸å®é™…å®ç°ä¸ä¸€è‡´")
            print("     - é¢å¤–çš„biaså‚æ•°æˆ–ç‰¹æ®Šå±‚")


if __name__ == "__main__":
    main()
