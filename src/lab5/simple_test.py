import torch
import numpy as np
import gc
import unittest
from transformers import AutoTokenizer, AutoModelForCausalLM
from modules import Qwen3Config, Qwen3ForCausalLM
from utils.utils import generate_text


# ANSIé¢œè‰²ä»£ç 
class Colors:
    RED = "\033[91m"
    GREEN = "\033[92m"
    YELLOW = "\033[93m"
    BLUE = "\033[94m"
    MAGENTA = "\033[95m"
    CYAN = "\033[96m"
    WHITE = "\033[97m"
    BOLD = "\033[1m"
    UNDERLINE = "\033[4m"
    END = "\033[0m"  # ç»“æŸé¢œè‰²


def load_custom_model():
    """åªåŠ è½½è‡ªå®šä¹‰æ¨¡å‹"""
    MODEL_PATH = "/ocean/model/Qwen3-8B"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.set_default_device(device)
    torch_dtype = torch.bfloat16

    custom_model = Qwen3ForCausalLM.from_pretrained(
        model_path=MODEL_PATH,
        device=device,
        torch_dtype=torch_dtype,
    )
    custom_model.eval()
    return custom_model, device, torch_dtype


def load_baseline_model():
    """åªåŠ è½½åŸºçº¿æ¨¡å‹"""
    MODEL_PATH = "/ocean/model/Qwen3-8B"
    device = "cuda" if torch.cuda.is_available() else "cpu"
    torch.set_default_device(device)
    torch_dtype = torch.bfloat16

    baseline_model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH,
        torch_dtype=torch_dtype,
        trust_remote_code=True,
    )
    baseline_model.model.to(device)
    baseline_model.eval()
    return baseline_model, device, torch_dtype


def load_tokenizer():
    """åŠ è½½tokenizer"""
    MODEL_PATH = "/ocean/model/Qwen3-8B"
    tokenizer = AutoTokenizer.from_pretrained(
        MODEL_PATH, trust_remote_code=True, use_fast=False
    )
    return tokenizer


def cleanup_model(model):
    """æ¸…ç†æ¨¡å‹å¹¶é‡Šæ”¾å†…å­˜"""
    if model is not None:
        del model

    # æ¸…ç†GPUç¼“å­˜
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()

    # å¼ºåˆ¶åƒåœ¾å›æ”¶
    gc.collect()


def generate_with_custom_model(model, tokenizer, prompt, max_length=20, device="cpu"):
    """ä½¿ç”¨è‡ªå®šä¹‰æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    return generate_text(
        model=model,
        tokenizer=tokenizer,
        prompt=prompt,
        max_length=max_length,
        temperature=0,
        device=device,
    )


def generate_with_baseline_model(model, tokenizer, prompt, max_length=20, device="cpu"):
    """ä½¿ç”¨transformersåŸºçº¿æ¨¡å‹ç”Ÿæˆæ–‡æœ¬"""
    return generate_text(
        model=model,
        tokenizer=tokenizer,
        prompt=prompt,
        max_length=max_length,
        temperature=0,
        device=device,
    )


def compare_model_outputs():
    """æ¯”è¾ƒä¸¤ä¸ªæ¨¡å‹çš„è¾“å‡ºä¸€è‡´æ€§"""
    # åŠ è½½tokenizerï¼ˆä¸¤ä¸ªæ¨¡å‹å…±ç”¨ï¼‰
    tokenizer = load_tokenizer()

    # æµ‹è¯•ç”¨çš„æç¤ºè¯
    test_prompts = [
        "Once upon a time in a land far, far away, there lived a dragon who",
        "The quick brown fox",
        "åœ¨ä¸€ä¸ªé˜³å…‰æ˜åªšçš„æ—©æ™¨",
        "To be or not to be",
    ]

    max_length = 10  # è¾ƒçŸ­çš„ç”Ÿæˆé•¿åº¦ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
    results = []
    custom_outputs = []
    baseline_outputs = []

    # ================ æµ‹è¯•è‡ªå®šä¹‰æ¨¡å‹ ================
    print(f"{Colors.BLUE}æ­£åœ¨åŠ è½½ç›®æ ‡æ¨¡å‹...{Colors.END}")
    custom_model, device, torch_dtype = load_custom_model()
    print(f"{Colors.BLUE}ç›®æ ‡æ¨¡å‹ç”Ÿæˆä¸­...{Colors.END}")
    for i, prompt in enumerate(test_prompts):
        custom_output = generate_with_custom_model(
            custom_model, tokenizer, prompt, max_length, device
        )
        custom_outputs.append(custom_output)
    # æ¸…ç†è‡ªå®šä¹‰æ¨¡å‹
    del custom_model
    print(f"{Colors.GREEN}ç›®æ ‡æ¨¡å‹æµ‹è¯•å®Œæˆ{Colors.END}")

    # ================ æµ‹è¯•åŸºçº¿æ¨¡å‹ ================
    print(f"{Colors.BLUE}æ­£åœ¨åŠ è½½åŸºçº¿æ¨¡å‹...{Colors.END}")
    baseline_model, device, torch_dtype = load_baseline_model()
    print(f"{Colors.BLUE}åŸºçº¿æ¨¡å‹ç”Ÿæˆä¸­...{Colors.END}")
    for i, prompt in enumerate(test_prompts):
        baseline_output = generate_with_baseline_model(
            baseline_model, tokenizer, prompt, max_length, device
        )
        baseline_outputs.append(baseline_output)
    # æ¸…ç†åŸºçº¿æ¨¡å‹
    cleanup_model(baseline_model)
    print(f"{Colors.GREEN}åŸºçº¿æ¨¡å‹æµ‹è¯•å®Œæˆ{Colors.END}")

    # æ¯”è¾ƒè¾“å‡º
    print("\n" + Colors.BOLD + "=" * 60 + Colors.END)
    print(Colors.BOLD + Colors.MAGENTA + " " * 20 + "æµ‹è¯•ç»“æœå¯¹æ¯”" + Colors.END)
    print(Colors.BOLD + "=" * 60 + Colors.END)

    for i, prompt in enumerate(test_prompts):
        is_consistent = custom_outputs[i] == baseline_outputs[i]
        results.append(
            {
                "prompt": prompt,
                "custom_output": custom_outputs[i],
                "baseline_output": baseline_outputs[i],
                "is_consistent": is_consistent,
            }
        )

        # ç¾åŒ–è¾“å‡º
        if is_consistent:
            status_icon = "âœ…"
            status_text = f"{Colors.GREEN}é€šè¿‡{Colors.END}"
        else:
            status_icon = "âŒ"
            status_text = f"{Colors.RED}å¤±è´¥{Colors.END}"

        print(f"\n{Colors.BOLD}æµ‹è¯•ç‚¹ {i+1}: {status_icon} {status_text}{Colors.END}")
        print(f"{Colors.CYAN}æç¤ºè¯:{Colors.END} {prompt}")
        print(f"{Colors.YELLOW}ç›®æ ‡æ¨¡å‹:{Colors.END} {custom_outputs[i]}")
        print(f"{Colors.YELLOW}åŸºçº¿æ¨¡å‹:{Colors.END} {baseline_outputs[i]}")
        if not is_consistent:
            print(f"{Colors.RED}âš ï¸  è¾“å‡ºä¸ä¸€è‡´{Colors.END}")
        print(Colors.WHITE + "-" * 50 + Colors.END)

    print(f"{Colors.GREEN}æ‰€æœ‰æµ‹è¯•å®Œæˆ{Colors.END}")

    return results


def main():
    """ä¸»å‡½æ•°"""
    print(f"{Colors.BOLD}{Colors.CYAN}å¼€å§‹æ¨¡å‹ä¸€è‡´æ€§æµ‹è¯•...{Colors.END}")

    try:
        results = compare_model_outputs()

        print("\n" + Colors.BOLD + "=" * 60 + Colors.END)
        print(Colors.BOLD + Colors.MAGENTA + " " * 20 + "æœ€ç»ˆæµ‹è¯•æ±‡æ€»" + Colors.END)
        print(Colors.BOLD + "=" * 60 + Colors.END)

        consistent_count = sum(1 for r in results if r["is_consistent"])
        total_count = len(results)

        print(
            f"{Colors.BOLD}ä¸€è‡´æ€§æµ‹è¯•ç»“æœ: {Colors.CYAN}{consistent_count}/{total_count}{Colors.END} {Colors.BOLD}é€šè¿‡{Colors.END}"
        )

        if consistent_count == total_count:
            print(
                f"{Colors.GREEN}{Colors.BOLD}ğŸ‰ æ­å–œï¼æ‰€æœ‰æµ‹è¯•éƒ½é€šè¿‡äº†ï¼æ¨¡å‹å®ç°æ­£ç¡®ï¼{Colors.END}"
            )
        else:
            print(
                f"{Colors.RED}{Colors.BOLD}âš ï¸  æœ‰ {total_count - consistent_count} ä¸ªæµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥æ¨¡å‹å®ç°{Colors.END}"
            )

    except Exception as e:
        print(f"{Colors.RED}æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}{Colors.END}")
    finally:
        # æœ€ç»ˆæ¸…ç†
        if torch.cuda.is_available():
            torch.cuda.empty_cache()
        gc.collect()


if __name__ == "__main__":
    main()
